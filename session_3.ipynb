{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: nltk in c:\\users\\hendra wijaya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hendra wijaya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hendra wijaya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hendra wijaya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hendra wijaya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hendra wijaya\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jalanin ini dulu!:\n",
    "- import nltk\n",
    "\n",
    "- nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenizing (preprocessing data)\n",
    "\n",
    "Tokenizing -> Cara AI memproses words yang ada dalam kalimat/paragraf. Proses tersebut kita namai sebagai tokenize. Tokenize tuh ibarat kita membagi text ke sebuah token yang mana token itu berupa (word, symbol seperti space dan sebagainya)\n",
    "\n",
    "Ada dua jenis Tokenizing:\n",
    "1. Word Tokenize: Membagi kalimat menjadi kata-kata. (word_tokenize)\n",
    "word tokenize membagi kata-kata jika dia ketemu spasi.\n",
    "\n",
    "2. Sentence Tokenize: Membagi kalimat menjadi kalimat-kalimat. (sent_tokenize)\n",
    "sent tokenize itu membagi kalimat sampai dia ketemu '.'\n",
    "\n",
    "Tokenize itu bisa paham panggilan kayak Mr. dan sebagainya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import tokenize dari library nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contoh cara pakai: word_tokenize(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Smith', \"'s\", 'train', 'does', 'not', 'leave', 'at', '12', 'AM', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Mr. Smith's train does not leave at 12 AM.\"\n",
    "list_word = word_tokenize(sentence)\n",
    "print(list_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contoh cara pakai: sent_tokenize(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Mr. Smith's train does not leave at 12 AM.\", 'He will be taking the 10:30 PM train.', 'He will arrive tomorrow at 5:00 AM.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Mr. Smith's train does not leave at 12 AM. He will be taking the 10:30 PM train. He will arrive tomorrow at 5:00 AM.\"\n",
    "list_sentence = sent_tokenize(sentence)\n",
    "print(list_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stop Words\n",
    "\n",
    "Menghilangkan kata-kata yang ga penting terhadap makna sebuah text. Contohnya \"I have an apple\" dimana kata \"an\" itu basically ga harus ada karena kita bisa tau kalau \"gw punya apple\" udah bisa tersampaikan tanpa adanya kata \"an\" yang basically tidak punya arti apapun.\n",
    "\n",
    "Intinya, stop words itu menghilangkan kata-kata yang tidak mempengaruhi makna dari kalimat tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "eng_stopwords = stopwords.words(\"english\") # parameternya adalah bahasa yang digunakan\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nah kata-kata yang ada disana tuh basically kata-kata yang dianggap ga penting sama si stopwords ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cara remove dari sentence yang kita punya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Smith', \"'s\", 'train', 'leave', '12', 'AM', '.']\n"
     ]
    }
   ],
   "source": [
    "removed_stopwords = []\n",
    "\n",
    "for word in list_word:\n",
    "    if word not in eng_stopwords:\n",
    "        removed_stopwords.append(word)\n",
    "\n",
    "print(removed_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metode list comprehension\n",
    "\n",
    "`word` paling depan itu kek variabel yang bakal dimasukin ke dalam list.\n",
    "\n",
    "beda sama `for word in list_word` yang bisa dilakuin langsung buat foreach, word disini yang ada di depan nunjukin bahwa word disini akan dipakai di dalam foreach yang pada akhirnya nanti akan dimasukkan ke dalam list `removed_stopwords`\n",
    "\n",
    "Jadi, misal aku punya code kek gini:\n",
    "```py\n",
    "removed_stopwords = []\n",
    "\n",
    "for word in list_word:\n",
    "    if word not in eng_stopwords:\n",
    "        removed_stopwords.append(word)\n",
    "```\n",
    "Nah, ini tuh sama aja kek\n",
    "```py\n",
    "removed_stopwords = [word for word in list_word if word not in eng_stopwords]\n",
    "```\n",
    "jadi, word yang paling depan itu tuh adalah variable yang akan di`append` ke list removed_stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Smith', \"'s\", 'train', 'leave', '12', '.']\n"
     ]
    }
   ],
   "source": [
    "removed_stopwords = [word for word\n",
    "                    in list_word if word.lower() not in eng_stopwords] # ini harus diubah ke lower case\n",
    "# karena stopwords dalam nltk adalah lowercase semua\n",
    "print(removed_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Smith', \"'s\", 'train', 'leave', '12']\n"
     ]
    }
   ],
   "source": [
    "# removing punctuation without stopword\n",
    "import string\n",
    "punctuation = string.punctuation\n",
    "\n",
    "removed_punctuation = [word for word\n",
    "                        in removed_stopwords if word not in punctuation]\n",
    "print(removed_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stemming\n",
    "\n",
    "Menghilangkan imbuhan (memakan -> makan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Buat dulu mereka ke dalam object \n",
    "karena PorterStemmer, SnowballStemmer, dan LancasterStemmer itu baru class doang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritma PorterStemmer itu lambat tapi akurasinya bagus dan dia cuman ada tapi dalam bahasa inggris.\n",
    "porter_stemmer = PorterStemmer() \n",
    "\n",
    "# SnowballStemmer itu lebih cepat dari PorterStemmer dan lebih banyak bahasa yang didukung.\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# LancasterStemmer itu lebih cepat dari SnowballStemmer tapi akurasinya kurang bagus.\n",
    "lancaster_stemmer = LancasterStemmer() # Algoritma cepat, akurasi kurang bagus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cara pakainya namaobj.stem(String variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writes:\n",
      "\tPorterStemmer: write\n",
      "\tSnowballStemmer: write\n",
      "\tLancasterStemmer: writ\n",
      "writing:\n",
      "\tPorterStemmer: write\n",
      "\tSnowballStemmer: write\n",
      "\tLancasterStemmer: writ\n",
      "written:\n",
      "\tPorterStemmer: written\n",
      "\tSnowballStemmer: written\n",
      "\tLancasterStemmer: writ\n",
      "wrote:\n",
      "\tPorterStemmer: wrote\n",
      "\tSnowballStemmer: wrote\n",
      "\tLancasterStemmer: wrot\n",
      "program:\n",
      "\tPorterStemmer: program\n",
      "\tSnowballStemmer: program\n",
      "\tLancasterStemmer: program\n",
      "programmer:\n",
      "\tPorterStemmer: programm\n",
      "\tSnowballStemmer: programm\n",
      "\tLancasterStemmer: program\n",
      "programming:\n",
      "\tPorterStemmer: program\n",
      "\tSnowballStemmer: program\n",
      "\tLancasterStemmer: program\n"
     ]
    }
   ],
   "source": [
    "words = [\"writes\", \"writing\", \"written\", \"wrote\", \"program\", \"programmer\", \"programming\"]\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word}:\")\n",
    "    print(f\"\\tPorterStemmer: {porter_stemmer.stem(word)}\")\n",
    "    print(f\"\\tSnowballStemmer: {snowball_stemmer.stem(word)}\")\n",
    "    print(f\"\\tLancasterStemmer: {lancaster_stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Lemmatizing\n",
    "Ngebalikin sebuah kata ke bentuk aslinya. Misal programmer yg udah distemming pakai lancaster malah jadi programm padahal programm dan programmer tuh kan beda ya nah itu kita balikin pakai lemmatizing.\n",
    "\n",
    "Dia tuh bisa menentukan konteks dari sebuah kata dan mengubahnya sesuai konteks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bikin objectnya dulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better:\n",
      "\tAdverb: good\n",
      "\tNoun: better\n",
      "\tVerb: better\n",
      "\tAdjective: well\n"
     ]
    }
   ],
   "source": [
    "word = \"better\"\n",
    "adverb = wordnet_lemmatizer.lemmatize(word, pos=\"a\")\n",
    "noun = wordnet_lemmatizer.lemmatize(word, pos=\"n\")\n",
    "verb = wordnet_lemmatizer.lemmatize(word, pos=\"v\")\n",
    "adjective = wordnet_lemmatizer.lemmatize(word, pos=\"r\")\n",
    "# dia nerima dua parameter yaitu \"word dan pos\" dimana pos itu adalah part of speech dari kata tersebut\n",
    "# a = adverb(keterangan), n = noun (kata benda), v = verb (kata kerja), r = adjective (kata sifat)\n",
    "\n",
    "print(f\"{word}:\")\n",
    "print(f\"\\tAdverb: {adverb}\")\n",
    "print(f\"\\tNoun: {noun}\")\n",
    "print(f\"\\tVerb: {verb}\")\n",
    "print(f\"\\tAdjective: {adjective}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes: Lemmatizing itu nerima parameter berupa per kata aja (string) bukan list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Part of Speech Tagging\n",
    "Menentukan kategori dari sebuah kata (adverb? noun? verb? adjective?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mr.', 'NNP'), ('Smith', 'NNP'), (\"'s\", 'POS'), ('train', 'NN'), ('leave', 'VBP'), ('12', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "tagged = pos_tag(removed_punctuation)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ini mksdnya apa kita bisa cek di https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/\n",
    "\n",
    "Contoh:\n",
    "- NNP -> Proper Noun Singular\n",
    "- POS -> Posesive Ending\n",
    "- NN -> Noun\n",
    "- VBP -> Verb Present Tense\n",
    "- CD -> Cardinal Digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Named Entity Recognition (NER)\n",
    "\n",
    "Pos-tagging tapi dia gambarin ke dalam sebuah tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = ne_chunk(tagged)\n",
    "# ner.draw() # <- kalo mo dijalanin cukup uncomment aja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Frequency Distribution\n",
    "dalam sebuah kalimat, kan ada kemungkinan sebuah kata-kata muncul lebih dari sekali. Nah, ini fungsinya buat ngitung aja kata ini ada berapa dan sebagainya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Buat jadi object dan constructornya nerima array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 2\n",
      "a: 2\n",
      "bowl: 2\n",
      "of: 2\n",
      "chicken: 2\n",
      "order: 1\n",
      "rice: 1\n",
      "and: 1\n",
      "ramen: 1\n",
      ",: 1\n",
      "but: 1\n",
      "hate: 1\n",
      "the: 1\n",
      ".: 1\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I order a bowl of chicken rice and a bowl of ramen, but I hate the chicken.\"\n",
    "fd = FreqDist(word_tokenize(sentence)) # Parameter ini nerima dalam bentuk array\n",
    "# Cara liat hasilnya adalah kita print pake for loop karena hasilnya berupa array\n",
    "for word, count in fd.most_common(): # Parameter most_common tuh nerima int kek 1, 2, 3, dst jadi kalau 3\n",
    "    # itu dia bakal nampilin 3 kata yang paling sering muncul\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "for word in fd:\n",
    "    print(f\"{word}: {fd[word]}\")\n",
    "```\n",
    "sama kek\n",
    "```py\n",
    "for word, count in fd.most_common(): \n",
    "    print(f\"{word}: {count}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
