{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python312\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python312\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Naives Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow:\n",
    "\n",
    "1. Read data:\n",
    "```py\n",
    "positive = open(\"positive.txt\", \"r\").read()\n",
    "negative = open(\"negative.txt\", \"r\").read()\n",
    "```\n",
    "\n",
    "2. Tokenize datanya\n",
    "Karena data yang diread akan dipindahkan ke dalam variable dalam tipe String. Kita harus tokenize dulu untuk bagian pre-processing datanya.\n",
    "```py\n",
    "list_words = word_tokenize(positive) + word_tokenize(negative)\n",
    "```\n",
    "\n",
    "3. Bersihin kata-kata\n",
    "\n",
    "- Stopwords\n",
    "```py\n",
    "list_words = [word for word in list_words if word.lower() not in eng_stopwords]\n",
    "```\n",
    "- Punctuation\n",
    "```py\n",
    "list_words = [word for word in list_words if word.lower() not in string.punctuation]\n",
    "```\n",
    "- Ngilangin angka\n",
    "```py\n",
    "list_words = [word for word in list_words if word.isalpha()] # append ke list_words kalau word adalah sebuah alphabet\n",
    "```\n",
    "\n",
    "4. Perkecil range kata-kata agar time complexitynya lebih cepat\n",
    "```py\n",
    "fd = FreqDist(list_words)\n",
    "```\n",
    "Nah, disini kita cukup ambil most common's wordnya dia aja\n",
    "```py\n",
    "list_words = [word.word for word in fd.most_common(100)] # ngambil 100 kata most-common aja.\n",
    "# word.word biar ngambil katanya doang.\n",
    "```\n",
    "\n",
    "5. Labeling sentences (pisahin kategori positif & negatif reviews)\n",
    "```py\n",
    "labeled_sentence = []\n",
    "for sentence in positive.split(\"\\n\"):\n",
    "    labeled_sentence.append((sentence, \"pos\"))\n",
    "\n",
    "for sentence in negative.split(\"\\n\"):\n",
    "    labeled_sentence.append((sentence, \"neg\"))\n",
    "```\n",
    "\n",
    "6. Membuat dataset (dari kata2 + labeled_sentence yang udah kita buat)\n",
    "```py\n",
    "dataset = []\n",
    "for sentence, label in labeled_sentence:\n",
    "    dict = {}\n",
    "    # word tokenize sentencesnya\n",
    "    words = word_tokenize(sentence)\n",
    "    # masukin ke dalam dictionarynya\n",
    "    for feature in list_words:\n",
    "        key = feature # kata\n",
    "        value = feature in words # kita mau cek aja sih ada ga sih kata-kata dalam kalimat kita yang ada dalam list of words bersih yang udah kita buat.\n",
    "        dict[key] = value # contoh {\"love\" : False, \"damn\" : True}\n",
    "    # kalau dictionarynya udah dibuat, kita masukin dia ke dataset yg udah kita buat\n",
    "    dataset.append((dict, label)) # masukin dictionary + labelnya (positive/negative)\n",
    "```\n",
    "Kenapa kayak gini? Karena kita mau bikin dataset yang isinya tuh kurang lebih kata-kata (ada/engganya) di kalimat yang positif/negatif. Karena balik lagi kita mau pake perhitungan bayes theorem yang mana kita akan hitung probabilitas setiap katanya.\n",
    "\n",
    "7. Shuffle kata-katanya karena isinya sekarang tuh pasti positif terus ke negatif.\n",
    "```py\n",
    "import random\n",
    "random.shuffle(dataset)\n",
    "```\n",
    "Kenapa kita shuffle? Karena pada dasarnya ketika kita training kita bakal bagi datasetnya berdasarkan berapa persen berapa persen kan, nah agar hasilnya lebih akurat, kita shuffle dulu.\n",
    "\n",
    "8. Bagi datasetnya\n",
    "```py \n",
    "counter = int(len(dataset) * 0.7) # pembagian dataset 70% training, 30% testing\n",
    "training_data = dataset[:counter]\n",
    "testing_data = dataset[counter:]\n",
    "```\n",
    "\n",
    "9. Lakukan algoritma naives bayes\n",
    "\n",
    "- Import dulu\n",
    "```py\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "```\n",
    "- Buat objectnya + masukin data trainingnya\n",
    "```py\n",
    "classify = NaiveBayesClassifier.train(training_data)\n",
    "```\n",
    "- Cek akurasi\n",
    "```py\n",
    "accuracy(classifier, testing_data) # dibanding dengan testing data kita\n",
    "```\n",
    "- Kali 100 buat dapet persentasenya\n",
    "\n",
    "Notes: tambahin most common word aja buat ningkatin akurasi. (semakin kotor dataset, semakin kecil persentase ke akuratannya)\n",
    "\n",
    "10. Buat modelnya\n",
    "\n",
    "- import picklenya\n",
    "```py\n",
    "import pickle\n",
    "```\n",
    "- open filenya\n",
    "```py\n",
    "file = open(\"model.pickle\", \"wb\")\n",
    "```\n",
    "- masukin data classifiernya ke dalam sebuah file\n",
    "```py\n",
    "pickle.dump(classifier, file)\n",
    "```\n",
    "\n",
    "11. Terus cara pake modelnya gimana?\n",
    "\n",
    "- open file picklenya pake mode \"rb\"\n",
    "```py\n",
    "file = open(\"model.pickle\", \"rb\")\n",
    "```\n",
    "- load ke dalam classifier\n",
    "```py\n",
    "classifier = pickle.load(file)\n",
    "```\n",
    "- Terakhir jangan lupa di close\n",
    "```py\n",
    "file.close()\n",
    "```\n",
    "- Kalau mau by-input bisa langsung\n",
    "```py\n",
    "review = input()\n",
    "words = word_tokenize(review)\n",
    "classifier = classifier.classify(FreqDist(words)) # dia minta parameternya words yang udah di freq dist.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
